# Kasparro â€” Agentic Facebook Performance Analyst

> **ğŸ¯ V2 High Bar Submission**: See [`README_V2.md`](./README_V2.md) for the complete V2 documentation with production-level enhancements.

An autonomous multi-agent system that diagnoses Facebook Ads performance issues with baseline/current comparisons, validates insights with structured evidence, and generates creative recommendations tightly linked to diagnosed issues.

## Quick Start

```bash
python -V  # should be >= 3.10
python -m venv .venv && source .venv/bin/activate  # win: .venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
python run.py
```

## Interactive Mode

Run without arguments for interactive query input:

```bash
python run.py

# You'll be prompted:
# ğŸ’¬ Enter your query: [type your question]
```

## Data

- **Full Dataset**: Place CSV at path specified in `config/config.yaml`
- **Sample Dataset**: Use `data/sample_fb_ads.csv` for testing
- **Documentation**: See `data/README.md` for column details

## Config

Edit `config/config.yaml`:

```yaml
python: "3.10"
random_seed: 42

llm:
  model: "gpt-4o"
  temperature: 0.3
  max_tokens: 2500

thresholds:
  confidence_min: 0.6
  low_ctr_threshold: 0.015
  low_roas_threshold: 2.0
  significant_change_pct: 0.15

data:
  full_csv: "data/synthetic_fb_ads_undergarments.csv"
  use_sample_data: false
```

## Repo Map

- `src/agents/` â€” planner.py, data_agent.py, insight_agent.py, evaluator.py, creative_generator.py
- `src/orchestrator/` â€” orchestrator.py (workflow coordination)
- `src/utils/` â€” config_loader.py, logger.py, schema.py
- `prompts/` â€” *.md prompt files with variable placeholders
- `reports/` â€” report_*.md, insights_*.json, creatives_*.json (timestamped)
- `logs/` â€” JSON execution traces
- `tests/` â€” Comprehensive test suite (7 test files, 46+ tests)
- `config/` â€” config.yaml
- `data/` â€” CSV datasets

## Run

```bash
# Direct query
python run.py

# Interactive mode
python run.py

# Using Makefile
make run QUERY="Which campaigns have low CTR?"
```

## Example Queries

```bash
# ROAS analysis
python run.py "Why did ROAS drop in the last week?"

# CTR investigation
python run.py "Which campaigns have low CTR and why?"

# Creative fatigue
python run.py "Identify creative fatigue in our campaigns"

# Platform comparison
python run.py "Compare Facebook vs Instagram performance"

# Comprehensive analysis
python run.py "Full performance audit: identify issues and recommend solutions"
```

## Outputs

All outputs are saved to the `reports/` directory:

- **`reports/report.md`** â€” Full analysis report with executive summary
- **`reports/insights.json`** â€” Structured hypotheses with confidence scores and evidence
- **`reports/creatives.json`** â€” Creative recommendations with specific variations

Execution logs saved to `logs/execution_*.json`

## ğŸ“Š Architecture

### Agent System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER QUERY                               â”‚
â”‚            "Analyze ROAS drop in last 7 days"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PLANNER AGENT                              â”‚
â”‚  â€¢ Decomposes query into subtasks                           â”‚
â”‚  â€¢ Identifies required metrics & analysis                   â”‚
â”‚  â€¢ Creates execution plan                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA AGENT                                â”‚
â”‚  â€¢ Loads Facebook Ads dataset                               â”‚
â”‚  â€¢ Performs quantitative analysis                           â”‚
â”‚  â€¢ Identifies trends & anomalies                            â”‚
â”‚  â€¢ Segments data by campaign/creative/audience              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INSIGHT AGENT                              â”‚
â”‚  â€¢ Generates hypotheses about performance                   â”‚
â”‚  â€¢ Explains patterns (fatigue, saturation, etc.)            â”‚
â”‚  â€¢ Identifies root causes                                   â”‚
â”‚  â€¢ Assigns preliminary confidence scores                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EVALUATOR AGENT                             â”‚
â”‚  â€¢ Validates hypotheses with quantitative evidence          â”‚
â”‚  â€¢ Tests statistical significance                           â”‚
â”‚  â€¢ Filters low-confidence insights                          â”‚
â”‚  â€¢ Produces validated insights                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CREATIVE GENERATOR AGENT                          â”‚
â”‚  â€¢ Identifies low-performing campaigns                      â”‚
â”‚  â€¢ Analyzes top-performing creative patterns               â”‚
â”‚  â€¢ Generates new creative recommendations                   â”‚
â”‚  â€¢ Provides specific headlines, messages, CTAs              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FINAL OUTPUTS                              â”‚
â”‚  â€¢ reports/report.md                                        â”‚
â”‚  â€¢ reports/insights.json                                    â”‚
â”‚  â€¢ reports/creatives.json                                   â”‚
â”‚  â€¢ logs/execution_*.json                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Principles

1. **Separation of Concerns**: Each agent has a single, well-defined responsibility
2. **Structured Prompts**: Prompts use Thinkâ†’Analyzeâ†’Conclude framework
3. **Validation Layer**: Evaluator agent critically tests all hypotheses
4. **Data Grounding**: All insights backed by quantitative evidence
5. **Quantitative Validation**: Statistical measures and confidence scoring

## ğŸ“ Repository Structure

```
kasparro-agentic-fb-analyst-bruuu/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ TESTING.md                   # Testing documentation
â”œâ”€â”€ SUBMISSION.md                # Assignment submission details
â”œâ”€â”€ requirements.txt             # Python dependencies (pinned)
â”œâ”€â”€ run.py                       # Main CLI entry point
â”œâ”€â”€ Makefile                     # Automation commands (test, lint, format, ci)
â”œâ”€â”€ pytest.ini                   # Pytest configuration
â”œâ”€â”€ .pre-commit-config.yaml      # Pre-commit hooks config
â”œâ”€â”€ .env.example                 # Environment template
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml              # CI/CD pipeline (GitHub Actions v4/v5)
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml             # Configuration (thresholds, paths, retries)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                 # Agent implementations
â”‚   â”‚   â”œâ”€â”€ base_agent.py      # Base agent class (with retry logic)
â”‚   â”‚   â”œâ”€â”€ planner.py         # Query decomposition
â”‚   â”‚   â”œâ”€â”€ data_agent.py      # Data loading & analysis
â”‚   â”‚   â”œâ”€â”€ insight_agent.py   # Hypothesis generation
â”‚   â”‚   â”œâ”€â”€ evaluator.py       # Hypothesis validation
â”‚   â”‚   â””â”€â”€ creative_generator.py  # Creative recommendations
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestrator/           # Workflow coordination
â”‚   â”‚   â””â”€â”€ orchestrator.py    # Main orchestrator
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # Utility functions
â”‚       â”œâ”€â”€ logger.py          # Enhanced logging (timing, errors, metadata)
â”‚       â”œâ”€â”€ config_loader.py   # Config management
â”‚       â””â”€â”€ schema.py          # Schema versioning & validation
â”‚
â”œâ”€â”€ prompts/                    # Prompt templates (.md)
â”‚   â”œâ”€â”€ planner_prompt.md
â”‚   â”œâ”€â”€ data_agent_prompt.md
â”‚   â”œâ”€â”€ insight_agent_prompt.md
â”‚   â”œâ”€â”€ evaluator_prompt.md
â”‚   â””â”€â”€ creative_generator_prompt.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ README.md              # Data documentation
â”‚   â””â”€â”€ synthetic_fb_ads_undergarments.csv  # Full dataset
â”‚
â”œâ”€â”€ reports/                    # Generated outputs
â”‚   â”œâ”€â”€ report_*.md            # Final markdown reports (timestamped)
â”‚   â”œâ”€â”€ insights_*.json        # Structured insights (timestamped)
â”‚   â””â”€â”€ creatives_*.json       # Creative recommendations (timestamped)
â”‚
â”œâ”€â”€ logs/                       # Execution logs
â”‚   â””â”€â”€ execution_*.json       # Structured JSON logs (session-based)
â”‚
â””â”€â”€ tests/                      # Test suite (46 tests)
    â”œâ”€â”€ conftest.py            # Shared test fixtures
    â”œâ”€â”€ test_data_agent.py     # Data agent tests (11 tests)
    â”œâ”€â”€ test_planner.py        # Planner tests (7 tests)
    â”œâ”€â”€ test_insight_agent.py  # Insight agent tests (7 tests)
    â”œâ”€â”€ test_evaluator.py      # Evaluator tests (3 tests)
    â”œâ”€â”€ test_creative_generator.py  # Creative generator tests (13 tests)
    â””â”€â”€ test_orchestrator.py   # Integration tests (5 tests)
```

## ğŸ“ˆ Data

The system analyzes Facebook Ads data with the following columns:

- **Identifiers**: campaign_name, adset_name, date
- **Performance**: spend, impressions, clicks, ctr, purchases, revenue, roas
- **Creative**: creative_type, creative_message
- **Targeting**: audience_type, platform, country

**Dataset**: ~4,500 rows covering Q1 2025

See `data/README.md` for more details.

## ğŸ“¤ Output Examples

### 1. Markdown Report (`reports/report.md`)

```markdown
# Facebook Ads Performance Analysis Report

## Query
Analyze ROAS drop in last 7 days

## Executive Summary
Analyzed 3 hypotheses and validated 3 key insights.

**Top Issues Identified:**
1. Low CTR campaigns are significantly contributing to ROAS decline (85%)
2. Video creatives underperform compared to UGC and Image (75%)

**Actionable Recommendations:** 2 campaigns identified for creative optimization.

## Key Findings
- Low CTR campaigns are significantly contributing to ROAS decline...
```

### 2. Insights JSON (`reports/insights.json`)

```json
{
  "query": "Analyze ROAS drop in last 7 days",
  "hypotheses": [
    {
      "hypothesis_id": "H1",
      "title": "Low CTR Campaigns are Driving ROAS Decline",
      "description": "...",
      "confidence": 0.85,
      "supporting_evidence": ["..."],
      "validation_status": "confirmed"
    }
  ],
  "evaluation": {
    "validated_count": 3,
    "confidence_threshold": 0.6
  }
}
```

### 3. Creatives JSON (`reports/creatives.json`)

```json
{
  "recommendations": [
    {
      "campaign_name": "Men Bold Colors Drop",
      "current_issue": "Low CTR due to lack of compelling hook",
      "creative_variations": [
        {
          "creative_type": "UGC",
          "headline": "Recommended by Athletes",
          "message": "See why men everywhere are switching...",
          "cta": "Shop Now",
          "expected_improvement": "+15% CTR"
        }
      ]
    }
  ]
}
```

## âœ… Validation

The Evaluator Agent validates hypotheses using:

1. **Quantitative Evidence**: Statistical measures from data
2. **Confidence Scoring**: 0.0-1.0 scale with thresholds
3. **Effect Magnitude**: Classify changes as large/medium/small
4. **Sample Size**: Ensure statistical validity
5. **Contradiction Detection**: Flag conflicting evidence

**Confidence Thresholds**:
- 0.8-1.0: Strong evidence âœ…
- 0.6-0.79: Moderate evidence âš ï¸
- <0.6: Rejected âŒ

## ğŸ§ª Testing

**Test Suite**: 46 tests with 100% pass rate

```bash
# Run all tests
make test

# Run with coverage
make test-coverage

# Run specific test file
pytest tests/test_data_agent.py -v

# Run CI checks locally
make ci
```

**Test Coverage**: 70-80% across all agents

See `TESTING.md` for comprehensive testing documentation.

## ğŸ” Observability & Reliability

The system includes comprehensive observability and error handling:

### Enhanced Logging
1. **Structured JSON Logs**: Detailed execution traces with session IDs
2. **Automatic Timing**: Duration tracking for all agent operations
3. **Error Tracking**: Full stack traces with context
4. **Color-Coded Console**: Visual log levels (INFO/WARNING/ERROR)
5. **Summary Statistics**: Performance metrics per session

### Retry Logic
1. **Exponential Backoff**: 3 automatic retries with increasing delays
2. **Smart Error Classification**: Retries only retryable errors
3. **Configurable Delays**: 1s â†’ 2s â†’ 4s (configurable in config.yaml)
4. **Full Logging**: All retry attempts logged with reasons

### Schema Validation
1. **Versioned Schemas**: All outputs include schema version (1.0.0)
2. **Drift Detection**: Automatically detect schema changes
3. **Validation**: Ensure outputs match expected structure
4. **Documentation**: Save schema definitions for reference

Optional Langfuse integration available (see config).

## ğŸ¯ Design Choices & Trade-offs

### 1. OpenAI GPT-4o (Instead of GPT-4 Turbo)
**Choice**: Optimized for speed with GPT-4o
- Temperature: 0.3 (focused, deterministic)
- Max tokens: 2500 (concise outputs)

**Rationale**: 60-75% faster (30-45s vs 2min), 70% cheaper, same quality

**Trade-off**: Slightly less creative, but more focused for analytical tasks

### 2. Sequential Agent Execution
**Choice**: Agents run sequentially in pipeline

**Rationale**: Clear dependencies, easier debugging, maintains state consistency

**Trade-off**: Could parallelize some sub-tasks for marginal gains

### 3. Data Summarization
**Choice**: Pass statistical summaries to agents, not full dataset

**Rationale**: Token efficiency, faster API calls, more focused analysis

**Trade-off**: Agents can't access raw data for deep-dive analysis

### 4. Structured Prompts with Templates
**Choice**: All prompts are template files with variable substitution

**Rationale**: Version control, reusability, separation of concerns

**Trade-off**: Less flexible for one-off customizations

## ğŸš§ Reproducibility

Ensured through:
- âœ… Pinned dependency versions (`requirements.txt`)
- âœ… Random seed configuration (`random_seed: 42`)
- âœ… Deterministic data processing
- âœ… Full execution logs
- âœ… Config-driven thresholds

## ğŸ“Š Performance

### Speed
- **Average Analysis Time**: 30-45 seconds
- **Breakdown**:
  - Data Loading: ~2s
  - Planner: ~5s
  - Data Agent: ~8s
  - Insight Agent: ~7s
  - Evaluator: ~8s
  - Creative Generator: ~10s
  - Report Generation: ~1s

### Quality
- **Hypothesis Validation Rate**: ~85%
- **Average Confidence Score**: ~77%
- **Creative Variations per Campaign**: 2-3

### Cost
- **Per Analysis**: ~$0.03
- **70% cheaper** than GPT-4 Turbo implementation

## ğŸ“‹ Production-Ready Improvements

Beyond the base requirements, the system now includes:

### P0 (Critical)
- âœ… **Comprehensive Testing**: 46 tests with 100% pass rate, 70-80% coverage
- âœ… **Enhanced Logging**: Timing, errors, metadata, session tracking
- âœ… **Error Handling**: Retry logic with exponential backoff
- âœ… **Failure Recovery**: Graceful degradation and fallbacks

### P1 (High Priority)
- âœ… **CI/CD Pipeline**: GitHub Actions with automated testing, linting, security scans
  - Test job: Python 3.10 & 3.11 with coverage reporting
  - Lint job: Flake8, Black, MyPy checks
  - Security job: Bandit security scanning
  - All using latest GitHub Actions (v4/v5)
- âœ… **Pre-commit Hooks**: Automatic code quality checks before commits
- âœ… **Code Quality**: Black (88 char), Flake8, isort, Bandit, MyPy
- âœ… **Code Formatting**: All code formatted with Black (standardized style)

### P2 (Nice to Have)
- âœ… **Schema Versioning**: Version tracking and drift detection
- âœ… **Developer Experience**: Comprehensive Makefile, documentation
- âœ… **Security Scanning**: Dependency and code security checks

See `TESTING.md` for detailed testing documentation and `SUBMISSION.md` for assignment details.

## ğŸ“‹ Base Assignment Requirements

All Kasparro assignment requirements met:

- âœ… **Agent Design**: All 5 agents (Planner, Data, Insight, Evaluator, Creative)
- âœ… **Deliverables**: insights.json, creatives.json, report.md, logs/
- âœ… **Structured Prompts**: Reasoning frameworks, JSON schemas
- âœ… **Validation Layer**: Quantitative hypothesis testing
- âœ… **Repository Structure**: Proper organization and documentation
- âœ… **Reproducibility**: Seeds, pinned versions, sample data
- âœ… **Testing**: Comprehensive test suite (46 tests)
- âœ… **Git Hygiene**: Clean commit history

## ğŸ“ Development Status

**Status**: Production-Ready âœ…
**CI/CD**: All checks passing
**Test Coverage**: 70-80%
**GitHub Actions**: Latest versions (v4/v5)

## ğŸ“ Support

For issues or questions:
1. Check logs in `logs/execution_*.json`
2. Verify config in `config/config.yaml`
3. Ensure OPENAI_API_KEY is set in `.env`
4. Review agent prompts in `prompts/`

## ğŸ“„ License

This is a technical assessment project for Kasparro.

---

**Built with**: Python 3.10+ | OpenAI GPT-4o | Multi-Agent Architecture
**Author**: Kandimalla Bruhadev
**Date**: December 2025
